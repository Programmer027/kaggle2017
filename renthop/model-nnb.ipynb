{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6bc9b087-c98b-bd97-66c6-92c5d01242d4",
    "deletable": true,
    "editable": true
   },
   "source": [
    "NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sklearn.cluster\n",
    "\n",
    "#import Levenshtein\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "#import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('fin-dprep-train.pkl')\n",
    "test_df = pd.read_pickle('fin-dprep-test.pkl')\n",
    "features_to_use = pickle.load(open('fin-dprep-flist.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "medium_price = pd.read_pickle('fin-medium-price.pkl')\n",
    "\n",
    "train_df = pd.merge(train_df, medium_price, left_on='listing_id', right_index=True)\n",
    "test_df = pd.merge(test_df, medium_price, left_on='listing_id', right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['predicted_price_diff'] = np.log(df.predicted_price) - np.log(df.price)\n",
    "    df['predicted_price_ratio'] = np.log(df.predicted_price) / np.log(df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_group 0.0488733992543\n",
      "price_ratio 0.0488733992543\n",
      "manager_shortdesc_rate 0.0688725887502\n",
      "manager_building0_rate 0.0688725887502\n",
      "manager_0feature_rate 0.0688725887502\n",
      "manager_median_price 0.0688725887502\n",
      "manager_lazy_rate 0.0688725887502\n"
     ]
    }
   ],
   "source": [
    "# fill in the NaN's.\n",
    "\n",
    "for t in train_df.keys():\n",
    "    nacount = train_df[t].isnull().sum()\n",
    "    if nacount:\n",
    "#        nacount_test = test_df[t].isnull().sum()\n",
    "        print(t, nacount / len(train_df))#, nacount_test / len(test_df))\n",
    "        \n",
    "train_df.fillna(0, inplace=True)\n",
    "test_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MeansProcessor:\n",
    "    def __init__(self, key, outkey = None, tgt = 'interest'):\n",
    "        self.key = key\n",
    "        self.outkey = key if outkey is None else outkey\n",
    "        \n",
    "        self.count = {}\n",
    "        self.means = {}\n",
    "        self.std = {}\n",
    "        self.global_means = {}\n",
    "        \n",
    "        self.tgt = tgt\n",
    "        \n",
    "        self.outkeys = [self.outkey + '_level', self.outkey + '_level_std']\n",
    "        \n",
    "    def fit(self, df):\n",
    "        self.global_means[self.outkey + '_level'] = df[self.tgt].mean()\n",
    "        self.global_means[self.outkey + '_level_std'] = df[self.tgt].std()\n",
    "            \n",
    "        for k in df.groupby(self.key, sort=False):\n",
    "            \n",
    "            self.count[k[0]] = len(k[1])\n",
    "\n",
    "            if len(k[1]) < 0:\n",
    "                self.means[k[0]] = np.nan\n",
    "                self.std[k[0]] = np.nan\n",
    "            else:\n",
    "                self.means[k[0]] = np.mean(k[1][self.tgt])\n",
    "                self.std[k[0]] = np.std(k[1][self.tgt])\n",
    "            \n",
    "    def predict(self, df, nans = False):\n",
    "        for l in self.outkeys:\n",
    "            df[l] = np.nan if nans else self.global_means[l]\n",
    "            \n",
    "        df[self.outkey + '_count'] = 0\n",
    "            \n",
    "        for k in df.groupby(self.key, sort=False):\n",
    "            if k[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            if k[0] in self.means:\n",
    "                df.loc[k[1].index, self.outkey + '_count'] = self.count[k[0]]\n",
    "                df.loc[k[1].index, self.outkey + '_level'] = self.means[k[0]]\n",
    "                df.loc[k[1].index, self.outkey + '_level_std'] = self.std[k[0]]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.outkeys.copy() + [self.outkey + '_count']\n",
    "\n",
    "# i kept the same index randomization (with fixed seed) so I could validate this code against\n",
    "# the original...\n",
    "\n",
    "target_num_map = {'low':0, 'medium':1, 'high':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "def proc_fold(fold):\n",
    "    train_index = fold[0]\n",
    "    test_index = fold[1]\n",
    "    \n",
    "    cv_train = train_df.iloc[train_index]\n",
    "    cv_valid = train_df.iloc[test_index][['interest_level', 'manager_id', 'building_id']]\n",
    "    cv_test = test_df.copy()\n",
    "    \n",
    "    m_build = MeansProcessor('building_id', 'building_sort')\n",
    "    m_build.fit(cv_train)\n",
    "    cv_valid = m_build.predict(cv_valid)\n",
    "    cv_test = m_build.predict(cv_test)\n",
    "\n",
    "    m_mgr = MeansProcessor('manager_id', 'manager_sort')\n",
    "    m_mgr.fit(cv_train)\n",
    "    cv_valid = m_mgr.predict(cv_valid)\n",
    "    cv_test = m_mgr.predict(cv_test)\n",
    "\n",
    "    m_comb = MeansProcessor(['building_id', 'manager_id'], 'mb_comb')\n",
    "    m_comb.fit(cv_train)\n",
    "    cv_valid = m_comb.predict(cv_valid)\n",
    "    cv_test = m_comb.predict(cv_test)\n",
    "\n",
    "    return cv_train, cv_valid, cv_test\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "folds = [(k[0], k[1]) for k in kf.split(list(range(train_df.shape[0])), train_y)]\n",
    "\n",
    "#with Pool(5) as pool:\n",
    "#    rv = pool.map(proc_fold, folds)\n",
    "\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    rv = pickle.load(open('bag-model-groupfeatures_nonan.pkl', 'rb'))\n",
    "except:\n",
    "    with Pool(5) as pool:\n",
    "        rv = pool.map(proc_fold, folds)\n",
    "\n",
    "        pickle.dump(rv, open('bag-model-groupfeatures_nonan.pkl', 'wb'))\n",
    "\n",
    "# dummies to get feature id's\n",
    "m_build = MeansProcessor('building_id', 'building_sort')\n",
    "m_mgr = MeansProcessor('manager_id', 'manager_sort')\n",
    "m_comb = MeansProcessor(['building_id', 'manager_id'], 'mb_comb')\n",
    "\n",
    "group_features = m_build.get_features() + m_mgr.get_features() + m_comb.get_features()\n",
    "\n",
    "#cv_test = [r[2] for r in rv]\n",
    "cv_test = []\n",
    "for r in rv:\n",
    "    cv_test.append(test_df.merge(r[2][group_features], left_index=True, right_index=True))\n",
    "\n",
    "cv_allvalid = pd.concat([r[1] for r in rv])\n",
    "\n",
    "train_df = train_df.merge(cv_allvalid[group_features], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "folds = [(k[0], k[1]) for k in kf.split(list(range(train_df.shape[0])), train_df.interest_cat)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for df in [train_df] + cv_test:\n",
    "    df['price_t'] = df['price_t'].clip(0, 13000)\n",
    "    df['price_per_room'] = df['price_per_room'].clip(0, 13000)\n",
    "#    df['density_lin005'] = df['density_lin005'].clip(-50, 50)\n",
    "    df['predicted_price_ratio'] = df['predicted_price_ratio'].clip(-50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.pos.dtype == 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_df_normalized = train_df.copy()\n",
    "cvtest_normalized = [df.copy() for df in cv_test]\n",
    "\n",
    "train_df_normalized['listing_id_norm'] = train_df_normalized['listing_id']\n",
    "for df in cvtest_normalized:\n",
    "    df['listing_id_norm'] = df['listing_id']\n",
    "\n",
    "normalized_keys = []\n",
    "\n",
    "scaler = {}\n",
    "for f in train_df.keys():\n",
    "    if f[0:2] == 'f_' or f[0:3] == 'fm_':\n",
    "        train_df_normalized[f] = train_df_normalized[f].clip(0, 1)\n",
    "        for df in cvtest_normalized:\n",
    "            df[f] = df[f].clip(0, 1)\n",
    "    elif 'interest' in f or f == 'listing_id' or f == 'index':\n",
    "        continue\n",
    "    elif f == 'created' or train_df[f].dtype == 'O':\n",
    "        train_df_normalized.drop(f, axis=1, inplace=True)\n",
    "        for df in cvtest_normalized:\n",
    "            df.drop(f, axis=1, inplace=True)\n",
    "        continue\n",
    "    else:\n",
    "        #print(f, train_df[f].min(), train_df[f].max(), test_df[f].min(), test_df[f].max())\n",
    "        scaler[f] = sklearn.preprocessing.StandardScaler()\n",
    "        train_df_normalized[f] = scaler[f].fit_transform(train_df_normalized[f].values.reshape(-1,1))[:,0]\n",
    "        for df in cvtest_normalized:\n",
    "            df[f] = scaler[f].transform(df[f].values.reshape(-1,1))[:,0]\n",
    "        \n",
    "    normalized_keys.append(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "models begin here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prep CV\n",
    "\n",
    "cv_train = []\n",
    "cv_valid = []\n",
    "\n",
    "for tr_index, val_index in kf.split(train_df.index, train_df.interest_cat):\n",
    "        cv_train.append(train_df_normalized.loc[tr_index])\n",
    "        cv_valid.append(train_df_normalized.loc[val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fl = normalized_keys.copy() # + m_build.get_features() + m_mgr.get_features() \n",
    "\n",
    "#for f in ['density_exp01', 'density_exp005', 'density_lin005', 'density_gaussian001', 'density_gaussian', 'density_gaussian01', 'density_gaussian02', 'density_gaussian04']:\n",
    "#    fl.remove(f)\n",
    "    \n",
    "#fl.append('density_gaussian02')\n",
    "#fl.append('density_exp01')\n",
    "\n",
    "\n",
    "fl.remove('predicted_price_ratio')\n",
    "fl.remove('manager_building0_rate')\n",
    "fl.remove('manager_shortdesc_rate')\n",
    "fl.remove('manager_0feature_rate')\n",
    "#fl.append('manager_sort_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, Conv2D, GlobalMaxPooling1D, GlobalMaxPooling2D, MaxPooling1D\n",
    "from keras.layers import Reshape\n",
    "import keras\n",
    "\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "#from keras.layers.recurrent import GRU\n",
    "from keras.layers import Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cv_train[0].interest\n",
    "\n",
    "def buildmodel(num_inputs, shape=[(32, .1), (16, .1)]):\n",
    "    layers = [Input(shape=(num_inputs,))]\n",
    "\n",
    "    for s in shape:\n",
    "        layers.append(Dense(s[0], activation='relu')(layers[-1]))\n",
    "        layers.append(Dropout(s[1])(layers[-1]))\n",
    "\n",
    "    output = Dense(3, activation='softmax', name='output')(layers[-1])\n",
    "\n",
    "    model = Model(inputs=layers[0], outputs=output)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_train[0][fl].values.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = buildmodel(num_inputs=cv_train[0][fl].values.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# plenty of code to do this, but it's simple enough\n",
    "def oneheat(y):\n",
    "    rv = np.zeros((len(y), 3))\n",
    "\n",
    "    for i in [0, 1, 2]:\n",
    "        rv[:,i] = (y == i)\n",
    "\n",
    "    return rv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39481 samples, validate on 9871 samples\n",
      "Epoch 1/120\n",
      "2s - loss: 0.6571 - val_loss: 0.5736\n",
      "Epoch 2/120\n",
      "1s - loss: 0.5670 - val_loss: 0.5530\n",
      "Epoch 3/120\n",
      "1s - loss: 0.5525 - val_loss: 0.5451\n",
      "Epoch 4/120\n",
      "1s - loss: 0.5436 - val_loss: 0.5424\n",
      "Epoch 5/120\n",
      "1s - loss: 0.5386 - val_loss: 0.5404\n",
      "Epoch 6/120\n",
      "1s - loss: 0.5351 - val_loss: 0.5373\n",
      "Epoch 7/120\n",
      "1s - loss: 0.5296 - val_loss: 0.5371\n",
      "Epoch 8/120\n",
      "1s - loss: 0.5258 - val_loss: 0.5352\n",
      "Epoch 9/120\n",
      "1s - loss: 0.5246 - val_loss: 0.5358\n",
      "Epoch 10/120\n",
      "1s - loss: 0.5217 - val_loss: 0.5348\n",
      "Epoch 11/120\n",
      "1s - loss: 0.5197 - val_loss: 0.5348\n",
      "Epoch 12/120\n",
      "1s - loss: 0.5159 - val_loss: 0.5346\n",
      "Epoch 13/120\n",
      "1s - loss: 0.5159 - val_loss: 0.5350\n",
      "Epoch 14/120\n",
      "1s - loss: 0.5117 - val_loss: 0.5347\n",
      "Epoch 15/120\n",
      "1s - loss: 0.5098 - val_loss: 0.5342\n",
      "Epoch 16/120\n",
      "1s - loss: 0.5078 - val_loss: 0.5339\n",
      "Epoch 17/120\n",
      "1s - loss: 0.5077 - val_loss: 0.5340\n",
      "Epoch 18/120\n",
      "1s - loss: 0.5044 - val_loss: 0.5357\n",
      "Epoch 19/120\n",
      "1s - loss: 0.5039 - val_loss: 0.5366\n",
      "Epoch 20/120\n",
      "1s - loss: 0.5014 - val_loss: 0.5352\n",
      "Epoch 21/120\n",
      "1s - loss: 0.5013 - val_loss: 0.5363\n",
      "Epoch 22/120\n",
      "1s - loss: 0.4992 - val_loss: 0.5357\n",
      "Epoch 23/120\n",
      "1s - loss: 0.4986 - val_loss: 0.5358\n",
      "Epoch 24/120\n",
      "1s - loss: 0.4978 - val_loss: 0.5365\n",
      "Epoch 25/120\n",
      "1s - loss: 0.4949 - val_loss: 0.5364\n",
      "Epoch 26/120\n",
      "1s - loss: 0.4949 - val_loss: 0.5378\n",
      "Epoch 27/120\n",
      "1s - loss: 0.4948 - val_loss: 0.5373\n",
      "Train on 39481 samples, validate on 9871 samples\n",
      "Epoch 1/120\n",
      "1s - loss: 0.6627 - val_loss: 0.5573\n",
      "Epoch 2/120\n",
      "0s - loss: 0.5746 - val_loss: 0.5379\n",
      "Epoch 3/120\n",
      "1s - loss: 0.5579 - val_loss: 0.5310\n",
      "Epoch 4/120\n",
      "0s - loss: 0.5489 - val_loss: 0.5283\n",
      "Epoch 5/120\n",
      "1s - loss: 0.5441 - val_loss: 0.5257\n",
      "Epoch 6/120\n",
      "1s - loss: 0.5394 - val_loss: 0.5238\n",
      "Epoch 7/120\n",
      "1s - loss: 0.5345 - val_loss: 0.5220\n",
      "Epoch 8/120\n",
      "0s - loss: 0.5327 - val_loss: 0.5209\n",
      "Epoch 9/120\n",
      "1s - loss: 0.5292 - val_loss: 0.5205\n",
      "Epoch 10/120\n",
      "0s - loss: 0.5265 - val_loss: 0.5205\n",
      "Epoch 11/120\n",
      "0s - loss: 0.5248 - val_loss: 0.5199\n",
      "Epoch 12/120\n",
      "0s - loss: 0.5215 - val_loss: 0.5195\n",
      "Epoch 13/120\n",
      "0s - loss: 0.5198 - val_loss: 0.5203\n",
      "Epoch 14/120\n",
      "1s - loss: 0.5172 - val_loss: 0.5184\n",
      "Epoch 15/120\n",
      "0s - loss: 0.5130 - val_loss: 0.5187\n",
      "Epoch 16/120\n",
      "0s - loss: 0.5111 - val_loss: 0.5181\n",
      "Epoch 17/120\n",
      "0s - loss: 0.5093 - val_loss: 0.5206\n",
      "Epoch 18/120\n",
      "0s - loss: 0.5081 - val_loss: 0.5190\n",
      "Epoch 19/120\n",
      "1s - loss: 0.5072 - val_loss: 0.5199\n",
      "Epoch 20/120\n",
      "0s - loss: 0.5049 - val_loss: 0.5195\n",
      "Epoch 21/120\n",
      "0s - loss: 0.5031 - val_loss: 0.5190\n",
      "Epoch 22/120\n",
      "1s - loss: 0.5045 - val_loss: 0.5192\n",
      "Epoch 23/120\n",
      "1s - loss: 0.5008 - val_loss: 0.5209\n",
      "Epoch 24/120\n",
      "1s - loss: 0.5002 - val_loss: 0.5214\n",
      "Epoch 25/120\n",
      "1s - loss: 0.4975 - val_loss: 0.5220\n",
      "Epoch 26/120\n",
      "1s - loss: 0.4966 - val_loss: 0.5260\n",
      "Epoch 27/120\n",
      "1s - loss: 0.4951 - val_loss: 0.5239\n",
      "Train on 39481 samples, validate on 9871 samples\n",
      "Epoch 1/120\n",
      "1s - loss: 0.6570 - val_loss: 0.5810\n",
      "Epoch 2/120\n",
      "0s - loss: 0.5691 - val_loss: 0.5611\n",
      "Epoch 3/120\n",
      "0s - loss: 0.5537 - val_loss: 0.5554\n",
      "Epoch 4/120\n",
      "1s - loss: 0.5443 - val_loss: 0.5520\n",
      "Epoch 5/120\n",
      "1s - loss: 0.5377 - val_loss: 0.5489\n",
      "Epoch 6/120\n",
      "1s - loss: 0.5322 - val_loss: 0.5455\n",
      "Epoch 7/120\n",
      "1s - loss: 0.5312 - val_loss: 0.5446\n",
      "Epoch 8/120\n",
      "0s - loss: 0.5272 - val_loss: 0.5441\n",
      "Epoch 9/120\n",
      "0s - loss: 0.5220 - val_loss: 0.5431\n",
      "Epoch 10/120\n",
      "0s - loss: 0.5208 - val_loss: 0.5444\n",
      "Epoch 11/120\n",
      "0s - loss: 0.5180 - val_loss: 0.5458\n",
      "Epoch 12/120\n",
      "1s - loss: 0.5173 - val_loss: 0.5440\n",
      "Epoch 13/120\n",
      "0s - loss: 0.5142 - val_loss: 0.5409\n",
      "Epoch 14/120\n",
      "0s - loss: 0.5109 - val_loss: 0.5415\n",
      "Epoch 15/120\n",
      "0s - loss: 0.5108 - val_loss: 0.5413\n",
      "Epoch 16/120\n",
      "1s - loss: 0.5099 - val_loss: 0.5403\n",
      "Epoch 17/120\n",
      "0s - loss: 0.5077 - val_loss: 0.5434\n",
      "Epoch 18/120\n",
      "0s - loss: 0.5059 - val_loss: 0.5415\n",
      "Epoch 19/120\n",
      "0s - loss: 0.5036 - val_loss: 0.5413\n",
      "Epoch 20/120\n",
      "0s - loss: 0.5023 - val_loss: 0.5427\n",
      "Epoch 21/120\n",
      "0s - loss: 0.5001 - val_loss: 0.5458\n",
      "Epoch 22/120\n",
      "0s - loss: 0.4998 - val_loss: 0.5439\n",
      "Epoch 23/120\n",
      "0s - loss: 0.4974 - val_loss: 0.5432\n",
      "Epoch 24/120\n",
      "0s - loss: 0.4966 - val_loss: 0.5431\n",
      "Epoch 25/120\n",
      "0s - loss: 0.4955 - val_loss: 0.5444\n",
      "Epoch 26/120\n",
      "0s - loss: 0.4932 - val_loss: 0.5451\n",
      "Epoch 27/120\n",
      "1s - loss: 0.4913 - val_loss: 0.5456\n",
      "Train on 39481 samples, validate on 9871 samples\n",
      "Epoch 1/120\n",
      "1s - loss: 0.6385 - val_loss: 0.5600\n",
      "Epoch 2/120\n",
      "1s - loss: 0.5715 - val_loss: 0.5442\n",
      "Epoch 3/120\n",
      "1s - loss: 0.5549 - val_loss: 0.5385\n",
      "Epoch 4/120\n",
      "0s - loss: 0.5459 - val_loss: 0.5364\n",
      "Epoch 5/120\n",
      "0s - loss: 0.5402 - val_loss: 0.5331\n",
      "Epoch 6/120\n",
      "0s - loss: 0.5364 - val_loss: 0.5316\n",
      "Epoch 7/120\n",
      "1s - loss: 0.5318 - val_loss: 0.5311\n",
      "Epoch 8/120\n",
      "1s - loss: 0.5290 - val_loss: 0.5290\n",
      "Epoch 9/120\n",
      "0s - loss: 0.5268 - val_loss: 0.5296\n",
      "Epoch 10/120\n",
      "0s - loss: 0.5230 - val_loss: 0.5271\n",
      "Epoch 11/120\n",
      "1s - loss: 0.5208 - val_loss: 0.5277\n",
      "Epoch 12/120\n",
      "1s - loss: 0.5185 - val_loss: 0.5268\n",
      "Epoch 13/120\n",
      "1s - loss: 0.5158 - val_loss: 0.5264\n",
      "Epoch 14/120\n",
      "0s - loss: 0.5146 - val_loss: 0.5264\n",
      "Epoch 15/120\n",
      "0s - loss: 0.5105 - val_loss: 0.5272\n",
      "Epoch 16/120\n",
      "0s - loss: 0.5102 - val_loss: 0.5285\n",
      "Epoch 17/120\n",
      "0s - loss: 0.5071 - val_loss: 0.5285\n",
      "Epoch 18/120\n",
      "0s - loss: 0.5055 - val_loss: 0.5275\n",
      "Epoch 19/120\n",
      "1s - loss: 0.5053 - val_loss: 0.5287\n",
      "Epoch 20/120\n",
      "0s - loss: 0.5021 - val_loss: 0.5274\n",
      "Epoch 21/120\n",
      "0s - loss: 0.5012 - val_loss: 0.5268\n",
      "Epoch 22/120\n",
      "0s - loss: 0.4993 - val_loss: 0.5289\n",
      "Epoch 23/120\n",
      "0s - loss: 0.4983 - val_loss: 0.5279\n",
      "Epoch 24/120\n",
      "1s - loss: 0.4948 - val_loss: 0.5287\n",
      "Train on 39484 samples, validate on 9868 samples\n",
      "Epoch 1/120\n",
      "1s - loss: 0.6558 - val_loss: 0.5653\n",
      "Epoch 2/120\n",
      "0s - loss: 0.5682 - val_loss: 0.5501\n",
      "Epoch 3/120\n",
      "0s - loss: 0.5525 - val_loss: 0.5442\n",
      "Epoch 4/120\n",
      "0s - loss: 0.5443 - val_loss: 0.5397\n",
      "Epoch 5/120\n",
      "0s - loss: 0.5408 - val_loss: 0.5383\n",
      "Epoch 6/120\n",
      "0s - loss: 0.5360 - val_loss: 0.5360\n",
      "Epoch 7/120\n",
      "0s - loss: 0.5317 - val_loss: 0.5352\n",
      "Epoch 8/120\n",
      "1s - loss: 0.5278 - val_loss: 0.5354\n",
      "Epoch 9/120\n",
      "0s - loss: 0.5260 - val_loss: 0.5346\n",
      "Epoch 10/120\n",
      "0s - loss: 0.5235 - val_loss: 0.5355\n",
      "Epoch 11/120\n",
      "0s - loss: 0.5218 - val_loss: 0.5355\n",
      "Epoch 12/120\n",
      "1s - loss: 0.5178 - val_loss: 0.5339\n",
      "Epoch 13/120\n",
      "1s - loss: 0.5168 - val_loss: 0.5341\n",
      "Epoch 14/120\n",
      "1s - loss: 0.5132 - val_loss: 0.5336\n",
      "Epoch 15/120\n",
      "0s - loss: 0.5108 - val_loss: 0.5340\n",
      "Epoch 16/120\n",
      "1s - loss: 0.5111 - val_loss: 0.5335\n",
      "Epoch 17/120\n",
      "0s - loss: 0.5081 - val_loss: 0.5328\n",
      "Epoch 18/120\n",
      "1s - loss: 0.5055 - val_loss: 0.5344\n",
      "Epoch 19/120\n",
      "1s - loss: 0.5032 - val_loss: 0.5357\n",
      "Epoch 20/120\n",
      "0s - loss: 0.5034 - val_loss: 0.5375\n",
      "Epoch 21/120\n",
      "1s - loss: 0.5015 - val_loss: 0.5351\n",
      "Epoch 22/120\n",
      "1s - loss: 0.4997 - val_loss: 0.5365\n",
      "Epoch 23/120\n",
      "0s - loss: 0.4991 - val_loss: 0.5364\n",
      "Epoch 24/120\n",
      "0s - loss: 0.4969 - val_loss: 0.5373\n",
      "Epoch 25/120\n",
      "0s - loss: 0.4950 - val_loss: 0.5387\n",
      "Epoch 26/120\n",
      "0s - loss: 0.4932 - val_loss: 0.5378\n",
      "Epoch 27/120\n",
      "0s - loss: 0.4943 - val_loss: 0.5382\n",
      "Epoch 28/120\n",
      "1s - loss: 0.4924 - val_loss: 0.5390\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "df_folds = []\n",
    "test_preds = []\n",
    "\n",
    "for fold in range(5):\n",
    "    m = buildmodel(num_inputs=cv_train[fold][fl].values.shape[1], shape=[(64, .2), (32, .1)])\n",
    "\n",
    "    bst_model_path = 'tmpnny.h5'\n",
    "\n",
    "    ES = keras.callbacks.EarlyStopping(patience=10)\n",
    "    MC = keras.callbacks.ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    tmp_train_x = cv_train[fold][fl].values\n",
    "    tmp_train_y = oneheat(cv_train[fold].interest_cat)\n",
    "\n",
    "    tmp_valid_x = cv_valid[fold][fl].values\n",
    "    tmp_valid_y = oneheat(cv_valid[fold].interest_cat)\n",
    "\n",
    "    test_x = cvtest_normalized[fold][fl].values\n",
    "    \n",
    "    history = m.fit(tmp_train_x, tmp_train_y, batch_size=256, epochs=120, verbose=2, validation_data=(tmp_valid_x, tmp_valid_y), callbacks=[MC, ES])\n",
    "    \n",
    "    m.load_weights(bst_model_path)\n",
    "\n",
    "    tpreds = m.predict(tmp_valid_x)\n",
    "\n",
    "    df_tmp = pd.DataFrame(tpreds)\n",
    "    df_tmp.set_index(cv_valid[fold].listing_id, inplace=True)\n",
    "\n",
    "    df_tmp.columns = [['low', 'medium', 'high']]\n",
    "#    df_tmp['listing_id'] = cv_valid[fold].listing_id\n",
    "    df_tmp['interest_cat'] = cv_valid[fold].interest_cat.values\n",
    "    #break\n",
    "\n",
    "    #print(log_loss(self.train_info.iloc[valid_idx].interest_level, df_tmp[self.tgts]))\n",
    "\n",
    "    df_folds.append(df_tmp)\n",
    "\n",
    "    test_preds.append(m.predict(test_x))\n",
    "\n",
    "    models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531062637395\n"
     ]
    }
   ],
   "source": [
    "df_cv = pd.concat(df_folds).sort_index()\n",
    "\n",
    "print(log_loss(df_cv.interest_cat, df_cv[['low', 'medium', 'high']]))\n",
    "\n",
    "testarray = np.array(test_preds.copy())\n",
    "\n",
    "tgts = ['low', 'medium', 'high']\n",
    "\n",
    "df_test = pd.DataFrame(testarray.mean(axis=0))\n",
    "df_test.columns = tgts\n",
    "df_test['listing_id'] = test_df.listing_id\n",
    "df_test.set_index('listing_id', inplace=True)\n",
    "\n",
    "df_output = pd.concat([df_cv[tgts], df_test])\n",
    "df_output.sort_index(inplace=True)\n",
    "\n",
    "df_output.to_pickle('bag-model-nn-v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_fold = []\n",
    "for f in range(testarray.shape[0]):\n",
    "    df_fold.append(pd.DataFrame(testarray[f]))\n",
    "    df_fold[-1]['listing_id'] = test_df.listing_id\n",
    "    df_fold[-1].sort_values('listing_id', inplace=True)\n",
    "    df_fold[-1].set_index('listing_id', inplace=True)\n",
    "\n",
    "pickle.dump((df_output, df_fold), open('model-nn.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 410,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
