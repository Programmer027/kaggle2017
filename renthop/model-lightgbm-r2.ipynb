{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6bc9b087-c98b-bd97-66c6-92c5d01242d4"
   },
   "source": [
    "lightgbm processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sklearn.cluster\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('fin-dprep-train.pkl')\n",
    "test_df = pd.read_pickle('fin-dprep-test.pkl')\n",
    "features_to_use = pickle.load(open('fin-dprep-flist.pkl', 'rb'))\n",
    "\n",
    "adams = pd.read_pickle('features-adams.pkl')\n",
    "\n",
    "train_df = pd.merge(train_df, adams, left_on='listing_id', right_index=True)\n",
    "test_df = pd.merge(test_df, adams, left_on='listing_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_num_map_reg = {'low':0, 'medium': (.5 + (9/13)) / 2, 'high':1}\n",
    "train_df['interest'] = np.array(train_df['interest_level'].apply(lambda x: target_num_map_reg[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medium_price = pd.read_pickle('fin-medium-price.pkl')\n",
    "\n",
    "train_df = pd.merge(train_df, medium_price, left_on='listing_id', right_index=True)\n",
    "test_df = pd.merge(test_df, medium_price, left_on='listing_id', right_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['predicted_price_diff'] = np.log(df.predicted_price) - np.log(df.price)\n",
    "    df['predicted_price_ratio'] = np.log(df.predicted_price) / np.log(df.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_group 0.0488733992543\n",
      "price_ratio 0.0488733992543\n",
      "manager_shortdesc_rate 0.0688725887502\n",
      "manager_building0_rate 0.0688725887502\n",
      "manager_0feature_rate 0.0688725887502\n",
      "manager_median_price 0.0688725887502\n",
      "manager_lazy_rate 0.0688725887502\n"
     ]
    }
   ],
   "source": [
    "# fill in the NaN's.\n",
    "\n",
    "for t in train_df.keys():\n",
    "    nacount = train_df[t].isnull().sum()\n",
    "    if nacount:\n",
    "#        nacount_test = test_df[t].isnull().sum()\n",
    "        print(t, nacount / len(train_df))#, nacount_test / len(test_df))\n",
    "        \n",
    "train_df.fillna(-99999, inplace=True)\n",
    "test_df.fillna(-99999, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeansProcessor:\n",
    "    def __init__(self, key, outkey = None, tgt = 'interest'):\n",
    "        self.key = key\n",
    "        self.outkey = key if outkey is None else outkey\n",
    "        \n",
    "        self.count = {}\n",
    "        self.means = {}\n",
    "        self.std = {}\n",
    "        self.global_means = {}\n",
    "        \n",
    "        self.tgt = tgt\n",
    "        \n",
    "        self.outkeys = [self.outkey + '_level', self.outkey + '_level_std']\n",
    "        \n",
    "    def fit(self, df):\n",
    "        self.global_means[self.outkey + '_level'] = df[self.tgt].mean()\n",
    "        self.global_means[self.outkey + '_level_std'] = df[self.tgt].std()\n",
    "            \n",
    "        for k in df.groupby(self.key, sort=False):\n",
    "            \n",
    "            self.count[k[0]] = len(k[1])\n",
    "\n",
    "            if len(k[1]) < 0:\n",
    "                self.means[k[0]] = np.nan\n",
    "                self.std[k[0]] = np.nan\n",
    "            else:\n",
    "                self.means[k[0]] = np.mean(k[1][self.tgt])\n",
    "                self.std[k[0]] = np.std(k[1][self.tgt])\n",
    "            \n",
    "    def predict(self, df, nans = False):\n",
    "        for l in self.outkeys:\n",
    "            df[l] = np.nan if nans else self.global_means[l]\n",
    "            \n",
    "        df[self.outkey + '_count'] = 0\n",
    "            \n",
    "        for k in df.groupby(self.key, sort=False):\n",
    "            if k[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            if k[0] in self.means:\n",
    "                df.loc[k[1].index, self.outkey + '_count'] = self.count[k[0]]\n",
    "                df.loc[k[1].index, self.outkey + '_level'] = self.means[k[0]]\n",
    "                df.loc[k[1].index, self.outkey + '_level_std'] = self.std[k[0]]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.outkeys.copy() + [self.outkey + '_count']\n",
    "\n",
    "# i kept the same index randomization (with fixed seed) so I could validate this code against\n",
    "# the original...\n",
    "\n",
    "target_num_map = {'low':0, 'medium':1, 'high':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "def proc_fold(fold):\n",
    "    train_index = fold[0]\n",
    "    test_index = fold[1]\n",
    "    \n",
    "    cv_train = train_df.iloc[train_index]\n",
    "    cv_valid = train_df.iloc[test_index][['interest_level', 'manager_id', 'building_id']]\n",
    "    cv_test = test_df.copy()\n",
    "    \n",
    "    m_build = MeansProcessor('building_id', 'building_sort')\n",
    "    m_build.fit(cv_train)\n",
    "    cv_valid = m_build.predict(cv_valid)\n",
    "    cv_test = m_build.predict(cv_test)\n",
    "\n",
    "    m_mgr = MeansProcessor('manager_id', 'manager_sort')\n",
    "    m_mgr.fit(cv_train)\n",
    "    cv_valid = m_mgr.predict(cv_valid)\n",
    "    cv_test = m_mgr.predict(cv_test)\n",
    "\n",
    "    m_comb = MeansProcessor(['building_id', 'manager_id'], 'mb_comb')\n",
    "    m_comb.fit(cv_train)\n",
    "    cv_valid = m_comb.predict(cv_valid)\n",
    "    cv_test = m_comb.predict(cv_test)\n",
    "\n",
    "    return cv_train, cv_valid, cv_test\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "folds = [(k[0], k[1]) for k in kf.split(list(range(train_df.shape[0])), train_y)]\n",
    "\n",
    "#with Pool(5) as pool:\n",
    "#    rv = pool.map(proc_fold, folds)\n",
    "\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    rv = pickle.load(open('bag-model-groupfeatures_nonan.pkl', 'rb'))\n",
    "except:\n",
    "    with Pool(5) as pool:\n",
    "        rv = pool.map(proc_fold, folds)\n",
    "\n",
    "        pickle.dump(rv, open('bag-model-groupfeatures_nonan.pkl', 'wb'))\n",
    "\n",
    "# dummies to get feature id's\n",
    "m_build = MeansProcessor('building_id', 'building_sort')\n",
    "m_mgr = MeansProcessor('manager_id', 'manager_sort')\n",
    "m_comb = MeansProcessor(['building_id', 'manager_id'], 'mb_comb')\n",
    "\n",
    "group_features = m_build.get_features() + m_mgr.get_features() + m_comb.get_features()\n",
    "\n",
    "#cv_test = [r[2] for r in rv]\n",
    "cv_test = []\n",
    "for r in rv:\n",
    "    cv_test.append(test_df.merge(r[2][group_features], left_index=True, right_index=True))\n",
    "\n",
    "cv_allvalid = pd.concat([r[1] for r in rv])\n",
    "\n",
    "train_df = train_df.merge(cv_allvalid[group_features], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new lightgbm tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "folds = [(k[0], k[1]) for k in kf.split(list(range(train_df.shape[0])), train_df.interest_cat)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prep CV\n",
    "\n",
    "cv_train = []\n",
    "cv_valid = []\n",
    "\n",
    "for tr_index, val_index in kf.split(train_df.index, train_df.interest_cat):\n",
    "        cv_train.append(train_df.loc[tr_index])\n",
    "        cv_valid.append(train_df.loc[val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fl = features_to_use.copy() + m_build.get_features() + m_mgr.get_features() \n",
    "\n",
    "#fl.append('manager_count')\n",
    "fl.append('manager_lazy_rate')\n",
    "#fl.append('predicted_price_ratio')\n",
    "\n",
    "fl.append('predicted_price')\n",
    "#fl.append('predicted_price_diff')\n",
    "\n",
    "fl.append('density_gaussian02')\n",
    "fl.append('density_exp01')\n",
    "\n",
    "#fl.remove('street_address')\n",
    "#fl.remove('half_bathroom')\n",
    "\n",
    "#fl.append('manager_sort_count')\n",
    "#fl.append('mb_comb_count')\n",
    "\n",
    "#fl.append('desc_xp_count')\n",
    "#fl.append('desc_xp_ratio')\n",
    "#fl.append('desc_xp_first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adams features\n",
    "\n",
    "fl.append('num_cap_share')\n",
    "fl.append('num_nr_of_lines')\n",
    "fl.append('num_redacted')\n",
    "fl.append('num_email')\n",
    "fl.append('num_phone_nr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for f in train_df.keys():\n",
    "    #print(f)\n",
    "    if 'rot' in f:\n",
    "        fl.append(f)\n",
    "        \n",
    "fl.append('num_rho')\n",
    "fl.append('num_phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t4_params = {\n",
    "    'boosting_type': 'gbdt', 'objective': 'multiclass', 'nthread': -1, 'silent': True,\n",
    "    'num_leaves': 2**5, 'learning_rate': 0.02, 'max_depth': -1, 'metric': ['multi_logloss'],\n",
    "    'max_bin': 255, 'subsample_for_bin': 50000,\n",
    "    'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.6, 'reg_alpha': 1, 'reg_lambda': 0,\n",
    "    'min_split_gain': 0.25, 'min_child_weight': .5, 'min_child_samples': 20, 'scale_pos_weight': 1}\n",
    "\n",
    "lgbm_params = t4_params.copy()\n",
    "lgbm_params['num_class'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.6,\n",
       " 'learning_rate': 0.02,\n",
       " 'max_bin': 255,\n",
       " 'max_depth': -1,\n",
       " 'metric': ['multi_logloss'],\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.5,\n",
       " 'min_split_gain': 0.25,\n",
       " 'nthread': -1,\n",
       " 'num_class': 3,\n",
       " 'num_leaves': 32,\n",
       " 'objective': 'multiclass',\n",
       " 'reg_alpha': 1,\n",
       " 'reg_lambda': 0,\n",
       " 'scale_pos_weight': 1,\n",
       " 'silent': True,\n",
       " 'subsample': 0.8,\n",
       " 'subsample_for_bin': 50000,\n",
       " 'subsample_freq': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.508155010505\n",
      "1 0.494528449743\n",
      "2 0.512507285305\n",
      "3 0.498951178481\n",
      "4 0.512042681769\n",
      "combined:  0.505236507453\n",
      "125.58150362968445\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# prep CV\n",
    "\n",
    "cv_train = []\n",
    "cv_valid = []\n",
    "\n",
    "for tr_index, val_index in kf.split(train_df.index, train_df.interest_cat):\n",
    "        cv_train.append(train_df.loc[tr_index])\n",
    "        cv_valid.append(train_df.loc[val_index])\n",
    "\n",
    "models = []\n",
    "cv_preds = []\n",
    "df_cvpreds = []\n",
    "\n",
    "for f in range(5):\n",
    "    dset = lgbm.Dataset(cv_train[f][fl], cv_train[f].interest_cat, silent=True)\n",
    "    dset_val = lgbm.Dataset(cv_valid[f][fl], cv_valid[f].interest_cat, silent=True)\n",
    "    models.append(lgbm.train(lgbm_params, dset, early_stopping_rounds=100, verbose_eval=False, valid_sets=dset_val, num_boost_round=2000))\n",
    "\n",
    "    #print('done training')\n",
    "    \n",
    "    cv_preds.append(models[-1].predict(cv_valid[f][fl], num_iteration=models[-1].best_iteration))\n",
    "\n",
    "    df_cvpreds.append(pd.DataFrame(cv_preds[f], columns=['low', 'medium', 'high']))\n",
    "    df_cvpreds[f].index = cv_valid[f].index\n",
    "    df_cvpreds[f]['interest_cat'] = cv_valid[f].interest_cat\n",
    "    df_cvpreds[f]['listing_id'] = cv_valid[f].listing_id\n",
    "\n",
    "    df_cvpreds[f].set_index('listing_id', inplace=True)\n",
    "    print(f, sklearn.metrics.log_loss(df_cvpreds[f].interest_cat, df_cvpreds[f][['low', 'medium', 'high']]))\n",
    "\n",
    "df_cvpreds = pd.concat(df_cvpreds)\n",
    "\n",
    "tgts = ['low', 'medium', 'high']\n",
    "\n",
    "print('combined: ', sklearn.metrics.log_loss(df_cvpreds.interest_cat, df_cvpreds[tgts]))\n",
    "\n",
    "end = time.time()\n",
    "print(end  - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 0.508595209192\n",
    "1 0.49459901602\n",
    "2 0.512629227307\n",
    "3 0.499392814962\n",
    "4 0.512765831483\n",
    "combined:  0.50559598398\n",
    "126.87185287475586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testpreds = []\n",
    "for i, m in enumerate(models):\n",
    "    testpreds.append(m.predict(cv_test[i][fl], num_iteration=m.best_iteration))\n",
    "    \n",
    "f = np.array(testpreds).mean(axis=0)\n",
    "df_testpreds = pd.DataFrame(f, columns=['low', 'medium', 'high'])\n",
    "df_testpreds.index = test_df.index\n",
    "df_testpreds['listing_id'] = test_df.listing_id\n",
    "df_testpreds.set_index('listing_id', inplace=True)\n",
    "\n",
    "df_output = pd.concat([df_testpreds, df_cvpreds[tgts]])\n",
    "df_output.sort_index(inplace=True)\n",
    "\n",
    "#df_output.to_pickle('bag-klightgbm-mgr.pkl')\n",
    "\n",
    "df_fold = []\n",
    "for i in range(len(testpreds)):\n",
    "    df_fold.append(pd.DataFrame(testpreds[i]))\n",
    "    df_fold[-1]['listing_id'] = test_df.listing_id\n",
    "    df_fold[-1].sort_values('listing_id', inplace=True)\n",
    "    df_fold[-1].set_index('listing_id', inplace=True)\n",
    "\n",
    "pickle.dump((df_output, df_fold), open('modeloutput-klightgbm-clf-r2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.241067085028\n",
      "1 0.236874368886\n",
      "2 0.241772109056\n",
      "3 0.239208746768\n",
      "4 0.244394229484\n",
      "combined:  0.240676286602\n",
      "54.50261068344116\n"
     ]
    }
   ],
   "source": [
    "# regressor\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "models = []\n",
    "cv_preds = []\n",
    "df_cvpreds = []\n",
    "\n",
    "t4_params = {\n",
    "    'boosting_type': 'gbdt', 'objective': 'regression_l2', 'nthread': -1, 'silent': True,\n",
    "    'num_leaves': 2**6, 'learning_rate': 0.02, 'max_depth': -1, 'metric': ['l2'],\n",
    "    'max_bin': 127, 'subsample_for_bin': 50000,\n",
    "    'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.6, 'reg_alpha': 1, 'reg_lambda': 0,\n",
    "    'min_split_gain': .01, 'min_child_weight': 1, 'min_child_samples': 20, 'scale_pos_weight': 1}\n",
    "\n",
    "lgbm_params = t4_params.copy()\n",
    "lgbm_params['num_class'] = 1\n",
    "\n",
    "for f in range(5):\n",
    "    dset = lgbm.Dataset(cv_train[f][fl], cv_train[f].interest, silent=True)\n",
    "    dset_val = lgbm.Dataset(cv_valid[f][fl], cv_valid[f].interest, silent=True)\n",
    "    models.append(lgbm.train(lgbm_params, dset, early_stopping_rounds=100, verbose_eval=False, valid_sets=dset_val, num_boost_round=2000))\n",
    "\n",
    "    #print('done training')\n",
    "    \n",
    "    cv_preds.append(models[-1].predict(cv_valid[f][fl]))\n",
    "\n",
    "    df_cvpreds.append(pd.DataFrame(cv_preds[f], columns=['prediction']))\n",
    "    df_cvpreds[f].index = cv_valid[f].index\n",
    "    df_cvpreds[f]['interest'] = cv_valid[f].interest\n",
    "    df_cvpreds[f]['listing_id'] = cv_valid[f].listing_id\n",
    "\n",
    "    df_cvpreds[f].set_index('listing_id', inplace=True)\n",
    "    print(f, np.sqrt(sklearn.metrics.mean_squared_error(cv_valid[f].interest, df_cvpreds[f].prediction)))\n",
    "\n",
    "df_cvpreds = pd.concat(df_cvpreds)\n",
    "\n",
    "tgts = ['low', 'medium', 'high']\n",
    "\n",
    "print('combined: ', np.sqrt(sklearn.metrics.mean_squared_error(df_cvpreds.interest, df_cvpreds.prediction)))\n",
    "\n",
    "end = time.time()\n",
    "print(end  - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 0.241634444613\n",
    "1 0.237429872346\n",
    "2 0.242643161794\n",
    "3 0.239873575947\n",
    "4 0.244510203144\n",
    "combined:  0.241230155962\n",
    "55.41301894187927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testpreds = []\n",
    "for i, m in enumerate(models):\n",
    "    testpreds.append(m.predict(cv_test[i][fl], num_iteration=m.best_iteration))\n",
    "    \n",
    "f = np.array(testpreds).mean(axis=0)\n",
    "df_testpreds = pd.DataFrame(f, columns=['prediction'])\n",
    "df_testpreds.index = test_df.index\n",
    "df_testpreds['listing_id'] = test_df.listing_id\n",
    "df_testpreds.set_index('listing_id', inplace=True)\n",
    "\n",
    "df_output = pd.concat([df_testpreds, df_cvpreds[['prediction']]])\n",
    "df_output.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "df_fold = []\n",
    "for i in range(len(testpreds)):\n",
    "    df_fold.append(pd.DataFrame(testpreds[i]))\n",
    "    df_fold[-1]['listing_id'] = test_df.listing_id\n",
    "    df_fold[-1].sort_values('listing_id', inplace=True)\n",
    "    df_fold[-1].set_index('listing_id', inplace=True)\n",
    "\n",
    "pickle.dump((df_output, df_fold), open('modeloutput-lightgbm-reg-r2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 410,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
