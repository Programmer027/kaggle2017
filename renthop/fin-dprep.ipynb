{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook handles the non-bayesian data prep.  I had a bit of a love/hate relationship with it, until I did out-of-bag testing and found it doesn't actually increase overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sklearn.cluster\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lightgbm can't handle NaN's, but with pd.fillna it's not *this* program's problem\n",
    "MISSING = np.nan\n",
    "# MISSING = -99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input data\n",
    "train_df=pd.read_json('../input/train.json')\n",
    "test_df=pd.read_json('../input/test.json')\n",
    "\n",
    "train_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determined to be broken after looking at results from a prior run of a submodel.  1025 is in it's Description.\n",
    "train_df.loc[train_df.listing_id == 7122037, 'price'] = 1025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_num_map = {'low':0, 'medium':1, 'high':2}\n",
    "train_df['interest_cat'] = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "target_num_map_reg = {'low':0, 'medium': (.5 + (9/13)) / 2.0, 'high':1}\n",
    "train_df['interest'] = np.array(train_df['interest_level'].apply(lambda x: target_num_map_reg[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "n_clusters = 92\n",
    "\n",
    "# modified from https://www.kaggle.com/luisblanche/two-sigma-connect-rental-listing-inquiries/price-compared-to-neighborhood-median/run/1011514\n",
    "\n",
    "def latlong_in_city(data):\n",
    "    return (data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)\n",
    "\n",
    "data_c=train_df[latlong_in_city(train_df)].copy()\n",
    "data_e=train_df[~latlong_in_city(train_df)].copy()\n",
    "\n",
    "coords_c=data_c.as_matrix(columns=['latitude', \"longitude\"])\n",
    "\n",
    "brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n",
    "\n",
    "brc.fit(coords_c)\n",
    "\n",
    "coords_tr=train_df.as_matrix(columns=['latitude', \"longitude\"])\n",
    "coords_te=test_df.as_matrix(columns=['latitude', \"longitude\"])\n",
    "\n",
    "train_df['location_cluster'] = brc.predict(coords_tr)\n",
    "test_df['location_cluster'] = brc.predict(coords_te)\n",
    "\n",
    "train_df.loc[~latlong_in_city(train_df), 'location_cluster'] = -1\n",
    "test_df.loc[~latlong_in_city(test_df), 'location_cluster'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "imean = train_df.interest.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.02912068367883\n"
     ]
    }
   ],
   "source": [
    "chisum = 0\n",
    "for key in ['location_cluster']:\n",
    "    for g in train_df.groupby(key):\n",
    "        if len(g[1]) > 20:\n",
    "            chi = ((g[1].interest.mean() - imean) ** 2.0) * len(g[1])\n",
    "            chisum += chi\n",
    "            #print(g[0], len(g[1]), chi, g[1].interest.mean())\n",
    "    \n",
    "print(chisum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ivanoliveri/two-sigma-connect-rental-listing-inquiries/new-features-from-addresses-fields\n",
    "\n",
    "def get_leven_ratio_row(row):\n",
    "    return Levenshtein.ratio(row.display_address.lower(), row.street_address.lower())\n",
    "\n",
    "def get_leven_ratio(df):\n",
    "    return df.apply(get_leven_ratio_row, axis=1)\n",
    "\n",
    "with Pool(2) as pool:\n",
    "    rv = pool.map(get_leven_ratio, [train_df, test_df])\n",
    "\n",
    "train_df['address_ratio'] = rv[0]\n",
    "test_df['address_ratio'] = rv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The infamous leak\n",
    "\n",
    "image_time = pd.read_csv('../input/listing_image_time.csv')\n",
    "\n",
    "train_df = pd.merge(train_df, image_time, left_on='listing_id', right_on='Listing_Id')\n",
    "test_df = pd.merge(test_df, image_time, left_on='listing_id', right_on='Listing_Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mostly from a few different public scripts\n",
    "\n",
    "def preproc_df(train_df):\n",
    "    #train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "    train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "    train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "    train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "    \n",
    "    train_df['half_bathroom'] = train_df['bathrooms'] - np.floor(train_df['bathrooms'])\n",
    "\n",
    "    train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "    train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "    train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    train_df[\"description_length\"] = train_df[\"description\"].apply(lambda x: len(x))\n",
    "\n",
    "    train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "    train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "    train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "    train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "    train_df[\"created_dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "    train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "    train_df[\"created_epoch\"] = pd.DatetimeIndex(train_df.created).astype(np.int64) // 1000000000\n",
    "    \n",
    "#    train_df[\"listing_div_day\"] = train_df[\"created\"].dt.hour\n",
    "\n",
    "    train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "    #train_df[\"pos2\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "\n",
    "    return train_df\n",
    "    \n",
    "train_df = preproc_df(train_df)    \n",
    "test_df = preproc_df(test_df)    \n",
    "    \n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_features = [\"address_ratio\", \"location_cluster\",\"bathrooms\", \"bedrooms\", \"half_bathroom\",\n",
    "                 \"latitude\", \"longitude\", \"price\",\"price_t\",\"price_per_room\", \"density\",\n",
    "                 \"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \n",
    "                 \"created_year\", \"created_dayofyear\", \"created_month\", \"created_day\", \"created_hour\", 'time_stamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved version of TF-IDF processing from https://www.kaggle.com/sudalairajkumar/two-sigma-connect-rental-listing-inquiries/xgb-starter-in-python\n",
    "\n",
    "My version does more precleaning and results in improved output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# correct punctuation etc for features/prep for TF-IDFing\n",
    "def featurefixer(l):\n",
    "    rv = ''\n",
    "    \n",
    "    for f in l:\n",
    "        s = f.lower()\n",
    "        s = s.replace(' ', '_')\n",
    "        s = s.replace('-', '_')\n",
    "        s = s.replace('/', '_')\n",
    "        s = s.replace('*', ' ')\n",
    "        \n",
    "        rv += s + ' '\n",
    "        \n",
    "    return rv\n",
    "\n",
    "\n",
    "train_df['features'] = train_df[\"features\"].apply(featurefixer)\n",
    "test_df['features'] = test_df[\"features\"].apply(featurefixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1    doorman elevator fitness_center cats_allowed d...\n",
      "2    laundry_in_building dishwasher hardwood_floors...\n",
      "3                              hardwood_floors no_fee \n",
      "4                                             pre_war \n",
      "Name: features, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=200, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df[\"features\"].head())\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tfidf.fit(train_df[\"features\"])\n",
    "#te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_train = tfidf.transform(train_df['features']).toarray()\n",
    "tfidf_test = tfidf.transform(test_df['features']).toarray()\n",
    "\n",
    "tfidf_fn = ['f_{0}'.format(f) for f in tfidf.get_feature_names()]\n",
    "\n",
    "pd_tfidf_train = pd.DataFrame(tfidf_train, columns=tfidf_fn, index=train_df.index)\n",
    "pd_tfidf_test = pd.DataFrame(tfidf_test, columns=tfidf_fn, index=test_df.index)\n",
    "\n",
    "# this was filtered out by chi squared measurements elsewhere... which might or might not be a great idea.  eh.\n",
    "\n",
    "mergelist = [\n",
    "#    [('f_central_ac', 'f_central_air', 'f_central_a_c'), 'f_central_ac'],\n",
    "#    [('f_hi_rise', 'f_highrise'), 'fm_highrise'],\n",
    "    [('f_ft_doorman', 'f_full_time_doorman',), 'f_doorman'],\n",
    "#    [('f_washer_in_unit', 'f_dryer_in_unit', 'f_washer_dryer_in_unit', 'f_in_unit_washer_dryer'), 'fm_laundry_in_unit'],\n",
    "#    [('f_concierge', 'f_concierge_service', 'f_24_7_concierge'), 'fm_concierge'],\n",
    "#    [('f_roofdeck', 'f_roof_deck', 'f_rooftop_deck'), 'fm_roofdeck'],\n",
    "    [('f_laundry_',), 'f_laundry'],\n",
    "#    [('f_washer_dryer_in_building', 'f_private_laundry_room_on_every_floor', 'f_on_site_laundry', 'f_laundry_room', 'f_laundry_in_building'), 'fm_laundry_in_building'],\n",
    "#    [('f_live_in_super',), 'f_live_in_superintendent'],\n",
    "#    [('f_prewar','f__ornate_prewar_details_'), 'f_pre_war'],\n",
    "    [('f_valet_services_including_dry_cleaning', 'f_valet_services'), 'f_valet'],\n",
    "    [('f_wheelchair_ramp',), 'f_wheelchair_access'],\n",
    "    [('f_high_ceiling',), 'f_high_ceilings'],\n",
    "    [('f_terraces___balconies',), 'f_terrace'],\n",
    "    [('f__dishwasher_',), 'f_dishwasher'],\n",
    "    [('f_decorative_fireplace', 'f_fireplaces'), 'f_fireplace'],\n",
    "#    [('f_common_parking_garage', 'f_full_service_garage', 'f_garage', 'f_on_site_garage'), 'fm_garage'],\n",
    "    [('f_private_outdoor_space', 'f_common_outdoor_space'), 'f_outdoor_space'],\n",
    "    [('f__elev_bldg_',), 'f_elevator']\n",
    "    \n",
    "]\n",
    "\n",
    "def run_mergelist(df, mergelist):\n",
    "    for m in mergelist:\n",
    "        #print(m, m[1])\n",
    "        \n",
    "        if m[1] not in df:\n",
    "            df[m[1]] = 0\n",
    "\n",
    "        for merge in m[0]:\n",
    "            #print('X ', merge, m[1])\n",
    "            df[m[1]] |= df[merge]\n",
    "\n",
    "            df.drop(merge, axis=1, inplace=True)\n",
    "                \n",
    "    return df\n",
    "            \n",
    "pd_tfidf_train = run_mergelist(pd_tfidf_train, mergelist)\n",
    "pd_tfidf_test = run_mergelist(pd_tfidf_test, mergelist)\n",
    "\n",
    "tfidf_fn = list(pd_tfidf_train.keys())\n",
    "\n",
    "train_df = pd.merge(train_df, pd_tfidf_train, left_index=True, right_index=True)\n",
    "test_df = pd.merge(test_df, pd_tfidf_test, left_index=True, right_index=True)\n",
    "\n",
    "train_df['d_lower'] = train_df.description.apply(lambda s: s.lower())\n",
    "test_df['d_lower'] = test_df.description.apply(lambda s: s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printchi(df, key):\n",
    "    imean = df.interest.mean()\n",
    "    for k in key:\n",
    "        subset = df[df[k] > 0]\n",
    "        chi = ((subset.interest.mean() - imean) ** 2.0) * len(subset)\n",
    "        print(k, len(subset), chi)\n",
    "\n",
    "\n",
    "descmap = [\n",
    "    [('exposed brick',), 'fd_exposed_brick'],\n",
    "    [('fireplace',), 'fd_fireplace'],\n",
    "    #[('doorman',), 'f_doorman'],\n",
    "    [('microwave',), 'fd_microwave'],\n",
    "    [('laundry in unit', 'washer dryer inside',), 'fd_laundry_in_unit'],\n",
    "    [('dishwasher',), 'fd_dishwasher'],\n",
    "    [('no fee',), 'fd_no_fee'],\n",
    "    [('subway',), 'fd_subway'],\n",
    "]\n",
    "\n",
    "fd_features = [m[1] for m in descmap]\n",
    "\n",
    "def backfill(df):\n",
    "    for m in descmap:\n",
    "        df[m[1]] = 0\n",
    "        for keyword in m[0]:\n",
    "            nv = df.d_lower.apply(lambda x: x.find(keyword) >= 0)\n",
    "            df[m[1]] |= nv\n",
    "\n",
    "        df[m[1]] = df[m[1]].astype(np.uint8)\n",
    "\n",
    "backfill(train_df)\n",
    "backfill(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            base_features.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# primitive version of bedroom/location price grouper (-submodel-medium does much more work than this!)\n",
    "train_high = train_df[train_df.interest_level == 'medium']\n",
    "\n",
    "price_group = {}\n",
    "\n",
    "for g in train_high.groupby(['bedrooms', 'location_cluster']):\n",
    "    if len(g[1]) < 10:\n",
    "        continue\n",
    "        \n",
    "    #print(g[0], g[1].price.mean())\n",
    "    price_group[g[0]] = g[1].price.mean()\n",
    "\n",
    "def apply_group(df):\n",
    "    df['price_group'] = MISSING\n",
    "    \n",
    "    for g in df.groupby(['bedrooms', 'location_cluster']):\n",
    "        if g[0] in price_group:\n",
    "            df.loc[g[1].index, 'price_group'] = price_group[g[0]]\n",
    "            \n",
    "    df['price_ratio'] = df['price'] / df['price_group']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = apply_group(train_df)\n",
    "test_df = apply_group(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# non-baynesian manager features.  this tries to pick out 'lazy' managers that don't write enough\n",
    "# descriptions and don't assign building ID's properly, etc.\n",
    "\n",
    "def apply_avg(df, means, key, clip = None):\n",
    "    df[key] = MISSING # can't use NaN's with lightgbm yet\n",
    "    \n",
    "    for m in means:\n",
    "        if clip is not None:\n",
    "            v = m[1] > clip\n",
    "            \n",
    "        df.loc[df['manager_id'] == m[0], key] = m[1]\n",
    "    \n",
    "means_dl = []\n",
    "means_b0 = []\n",
    "means_f0 = []\n",
    "median_price = []\n",
    "\n",
    "for m in train_df[['manager_id', 'description_length', 'building_id', 'num_features', 'price']].groupby('manager_id'):\n",
    "    if len(m[1]) < 5:\n",
    "        continue\n",
    "        \n",
    "    means_dl.append((m[0], (m[1].description_length <= 8).mean()))\n",
    "    means_b0.append((m[0], (m[1].building_id == 0).mean()))\n",
    "    means_f0.append((m[0], (m[1].num_features == 0).mean()))\n",
    "    median_price.append((m[0], m[1].price.median()))\n",
    "    \n",
    "for df in [train_df, test_df]:\n",
    "    apply_avg(df, means_dl, 'manager_shortdesc_rate', 0.25)\n",
    "    apply_avg(df, means_b0, 'manager_building0_rate', 0.77)\n",
    "    apply_avg(df, means_f0, 'manager_0feature_rate')\n",
    "    apply_avg(df, median_price, 'manager_median_price')\n",
    "    df['manager_lazy_rate'] = np.clip(df.manager_shortdesc_rate + df.manager_building0_rate, 0, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did a few different kernel density functions, and settled on these two as most effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "kde01e = KernelDensity(kernel='exponential', bandwidth=0.01).fit(train_df[['latitude', 'longitude']].values)\n",
    "\n",
    "train_df['density_exp01'] = kde01e.score_samples(train_df[['latitude', 'longitude']].values)\n",
    "test_df['density_exp01'] = kde01e.score_samples(test_df[['latitude', 'longitude']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kde01 = KernelDensity(kernel='gaussian', bandwidth=0.02).fit(train_df[['latitude', 'longitude']].values)\n",
    "\n",
    "train_df['density_gaussian02'] = kde01.score_samples(train_df[['latitude', 'longitude']].values)\n",
    "test_df['density_gaussian02'] = kde01.score_samples(test_df[['latitude', 'longitude']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle('fin-dprep-train.pkl')\n",
    "test_df.to_pickle('fin-dprep-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_features = base_features + tfidf_fn + fd_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(full_features, open('fin-dprep-flist.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
