{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stacking code includes XGB and lightGBM stackers - I found using OOB tests (not posted here, I wrote them in a hurry and this is pretty much the same code) that XGB alone worked better - but they usually work in lock step, so I can experiment with the much faster lightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sklearn.cluster\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('fin-dprep-train.pkl')\n",
    "test_df = pd.read_pickle('fin-dprep-test.pkl')\n",
    "\n",
    "features_to_use = pickle.load(open('fin-dprep-flist.pkl', 'rb'))\n",
    "\n",
    "medium_price = pd.read_pickle('fin-medium-price.pkl')\n",
    "\n",
    "train_df = pd.merge(train_df, medium_price, left_on='listing_id', right_index=True)\n",
    "test_df = pd.merge(test_df, medium_price, left_on='listing_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_num_map = {'low':0, 'medium':1, 'high':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "folds = [(k[0], k[1]) for k in kf.split(list(range(train_df.shape[0])), train_y)]\n",
    "\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "\n",
    "for dev_index, val_index in kf.split(range(train_df.shape[0]), train_df.interest_cat):\n",
    "    train_ids.append(train_df.iloc[dev_index].listing_id.values)\n",
    "    val_ids.append(train_df.iloc[val_index].listing_id.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackerXGB:\n",
    "    def __init__(self, use, nn_shape = [(32, .1), (16, .1)]):\n",
    "        self.train_info, self.test_ids = pd.read_pickle('stacker-info.pkl')\n",
    "        self.nfolds = 5\n",
    "        \n",
    "        self.nn_shape = nn_shape.copy()\n",
    "        \n",
    "        df_nn = use[0][0].copy()\n",
    "        self.df_nn_tests = [u.copy() for u in use[0][1]]\n",
    "\n",
    "        for i, df in enumerate(use[1:]):\n",
    "            df_nn = pd.merge(df_nn, df[0], left_index = True, right_index = True)\n",
    "            for f in range(self.nfolds):\n",
    "                self.df_nn_tests[f] = pd.merge(self.df_nn_tests[f], df[1][f], left_index = True, right_index = True)\n",
    "\n",
    "        self.df_nn_train = df_nn.loc[self.train_info.index]\n",
    "        \n",
    "        #self.test_x = np.array(self.df_nn_test.values)\n",
    "        \n",
    "        self.models = []\n",
    "        self.df_folds = []\n",
    "        self.test_preds = []\n",
    "        \n",
    "        self.tgts = ['low', 'medium', 'high']\n",
    "                \n",
    "        param = {}\n",
    "        param['objective'] = 'multi:softprob'\n",
    "        #param['tree_method'] = 'hist'\n",
    "        param['eta'] = 0.05\n",
    "        param['eta'] = 0.01\n",
    "        param['max_depth'] = 3\n",
    "        param['silent'] = 1\n",
    "        param['num_class'] = 3\n",
    "        param['eval_metric'] = \"mlogloss\"\n",
    "        param['min_child_weight'] = 4\n",
    "        param['subsample'] = .7\n",
    "        param['colsample_bytree'] = 0.7\n",
    "        param['seed'] = 1234\n",
    "\n",
    "        self.plst = list(param.items())\n",
    "\n",
    "    # plenty of code to do this, but it's simple enough\n",
    "    def oneheat(self, y):\n",
    "        rv = np.zeros((len(y), 3))\n",
    "\n",
    "        for i in [0, 1, 2]:\n",
    "            rv[:,i] = (y == i)\n",
    "\n",
    "        return rv\n",
    "    \n",
    "\n",
    "    def run_fold(self, foldnum, train_idx, valid_idx):\n",
    "        \n",
    "        nn_fold_train = self.df_nn_train.loc[train_idx]\n",
    "        nn_fold_valid = self.df_nn_train.loc[valid_idx]\n",
    "\n",
    "        tmp_train_x = np.array(nn_fold_train.values)\n",
    "        tmp_valid_x = np.array(nn_fold_valid.values)\n",
    "\n",
    "        xgtrain = xgb.DMatrix(tmp_train_x, label=self.train_info.loc[train_idx].interest_level)\n",
    "        xgvalid = xgb.DMatrix(tmp_valid_x, label=self.train_info.loc[valid_idx].interest_level)\n",
    "\n",
    "        watchlist = [ (xgtrain,'train'), (xgvalid, 'valid') ]\n",
    "        model = xgb.train(self.plst, xgtrain, 8000, watchlist, early_stopping_rounds=50, verbose_eval=100)\n",
    "        \n",
    "        tpreds = model.predict(xgvalid, ntree_limit = model.best_ntree_limit)\n",
    "\n",
    "        df_tmp = pd.DataFrame(tpreds)\n",
    "\n",
    "        df_tmp.columns = [['low', 'medium', 'high']]\n",
    "        df_tmp['listing_id'] = nn_fold_valid.index\n",
    "        df_tmp.set_index('listing_id', inplace=True)\n",
    "\n",
    "        #tgts = ['low', 'medium', 'high']\n",
    "        print(log_loss(self.train_info.loc[valid_idx].interest_level, df_tmp[self.tgts]))\n",
    "\n",
    "        self.df_folds.append(df_tmp)\n",
    "        \n",
    "        test_x = xgb.DMatrix(self.df_nn_tests[foldnum].values)\n",
    "        self.test_preds.append(model.predict(test_x, ntree_limit = model.best_ntree_limit))\n",
    "        \n",
    "        self.models.append(model)\n",
    "        \n",
    "        return df_tmp\n",
    "    \n",
    "        \n",
    "    def run(self, train_ids, val_ids):        \n",
    "        #print(folds)\n",
    "        \n",
    "        #self.kf_nn = model_selection.StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        #self.folds_nn = [(k[0], k[1]) for k in self.kf_nn.split(self.df_nn_train.index, self.train_info.interest_level)]\n",
    "        \n",
    "        for i, fold in enumerate(zip(train_ids, val_ids)):\n",
    "            self.run_fold(i, fold[0], fold[1])\n",
    "        \n",
    "        self.df_cv = pd.concat(self.df_folds).sort_index()\n",
    "\n",
    "        print('CV logloss:', log_loss(self.train_info.interest_level, self.df_cv[self.tgts]))\n",
    "\n",
    "        testarray = np.array(self.test_preds.copy())\n",
    "\n",
    "        self.df_test = pd.DataFrame(testarray.mean(axis=0))\n",
    "        self.df_test.columns = [['low', 'medium', 'high']]\n",
    "        self.df_test['listing_id'] = self.test_ids\n",
    "        self.df_test.set_index('listing_id', inplace=True)\n",
    "\n",
    "        return self.df_cv, self.df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "class StackerLGBM:\n",
    "    def __init__(self, use, nn_shape = [(32, .1), (16, .1)], nfolds = 5):\n",
    "        self.train_info, self.test_ids = pd.read_pickle('stacker-info.pkl')\n",
    "        self.nfolds = 5\n",
    "        \n",
    "        self.nn_shape = nn_shape.copy()\n",
    "        \n",
    "        df_nn = use[0][0].copy()\n",
    "        self.df_nn_tests = [u.copy() for u in use[0][1]]\n",
    "\n",
    "        for i, df in enumerate(use[1:]):\n",
    "            df_nn = pd.merge(df_nn, df[0], left_index = True, right_index = True)\n",
    "            for f in range(self.nfolds):\n",
    "                self.df_nn_tests[f] = pd.merge(self.df_nn_tests[f], df[1][f], left_index = True, right_index = True)\n",
    "\n",
    "        self.df_nn_train = df_nn.loc[self.train_info.index]\n",
    "        \n",
    "        #self.test_x = np.array(self.df_nn_test.values)\n",
    "        \n",
    "        self.models = []\n",
    "        self.df_folds = []\n",
    "        self.test_preds = []\n",
    "        \n",
    "        self.tgts = ['low', 'medium', 'high']\n",
    "        \n",
    "        t4_params = {\n",
    "            'boosting_type': 'gbdt', 'objective': 'multiclass', 'nthread': -1, 'silent': True,\n",
    "            'num_leaves': 5, 'learning_rate': 0.01, 'max_depth': -1, 'metric': ['multi_logloss'],\n",
    "            'max_bin': 255, 'subsample_for_bin': 50000,\n",
    "            'subsample': 0.7, 'subsample_freq': 1, 'colsample_bytree': .85, 'reg_alpha': 1, 'reg_lambda': 0,\n",
    "            'min_split_gain': 1, 'min_child_weight': 1, 'min_child_samples': 50, 'scale_pos_weight': 1}\n",
    "\n",
    "        \n",
    "        self.lgbm_params = t4_params.copy()\n",
    "        self.lgbm_params['num_class'] = 3\n",
    "\n",
    "\n",
    "    # plenty of code to do this, but it's simple enough\n",
    "    def oneheat(self, y):\n",
    "        rv = np.zeros((len(y), 3))\n",
    "\n",
    "        for i in [0, 1, 2]:\n",
    "            rv[:,i] = (y == i)\n",
    "\n",
    "        return rv\n",
    "    \n",
    "\n",
    "    def run_fold(self, foldnum, train_idx, valid_idx):\n",
    "        \n",
    "        nn_fold_train = self.df_nn_train.loc[train_idx]\n",
    "        nn_fold_valid = self.df_nn_train.loc[valid_idx]\n",
    "\n",
    "        tmp_train_x = np.array(nn_fold_train.values)\n",
    "        tmp_valid_x = np.array(nn_fold_valid.values)\n",
    "\n",
    "        #xgtrain = xgb.DMatrix(tmp_train_x, label=self.train_info.iloc[train_idx].interest_level)\n",
    "        #xgvalid = xgb.DMatrix(tmp_valid_x, label=self.train_info.iloc[valid_idx].interest_level)\n",
    "        \n",
    "        dset = lgbm.Dataset(tmp_train_x, self.train_info.loc[train_idx].interest_level, silent=True)\n",
    "        dset_val = lgbm.Dataset(tmp_valid_x, self.train_info.loc[valid_idx].interest_level, silent=True)\n",
    "\n",
    "        #watchlist = [ (xgtrain,'train'), (xgvalid, 'valid') ]\n",
    "        #model = xgb.train(self.plst, xgtrain, 4000, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "\n",
    "        model = lgbm.train(self.lgbm_params, dset, early_stopping_rounds=100, verbose_eval=False, valid_sets=dset_val, num_boost_round=2000)\n",
    "        \n",
    "        tpreds = model.predict(tmp_valid_x, num_iteration=model.best_iteration)\n",
    "\n",
    "        df_tmp = pd.DataFrame(tpreds)\n",
    "\n",
    "        df_tmp.columns = [['low', 'medium', 'high']]\n",
    "        df_tmp['listing_id'] = nn_fold_valid.index\n",
    "        df_tmp.set_index('listing_id', inplace=True)\n",
    "\n",
    "        #tgts = ['low', 'medium', 'high']\n",
    "        print(log_loss(self.train_info.loc[valid_idx].interest_level, df_tmp[self.tgts]))\n",
    "\n",
    "        self.df_folds.append(df_tmp)\n",
    "        \n",
    "        self.test_preds.append(model.predict(self.df_nn_tests[foldnum].values, num_iteration=model.best_iteration))\n",
    "        \n",
    "        self.models.append(model)\n",
    "        \n",
    "        return df_tmp\n",
    "    \n",
    "    def run(self, train_ids, val_ids):        \n",
    "        #print(folds)\n",
    "        \n",
    "        #self.kf_nn = model_selection.StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "        #self.folds_nn = [(k[0], k[1]) for k in self.kf_nn.split(self.df_nn_train.index, self.train_info.interest_level)]\n",
    "        \n",
    "        for i, fold in enumerate(zip(train_ids, val_ids)):\n",
    "            self.run_fold(i, fold[0], fold[1])\n",
    "        \n",
    "        self.df_cv = pd.concat(self.df_folds).sort_index()\n",
    "\n",
    "        print('CV logloss:', log_loss(self.train_info.interest_level, self.df_cv[self.tgts]))\n",
    "\n",
    "        testarray = np.array(self.test_preds.copy())\n",
    "\n",
    "        self.df_test = pd.DataFrame(testarray.mean(axis=0))\n",
    "        self.df_test.columns = [['low', 'medium', 'high']]\n",
    "        self.df_test['listing_id'] = self.test_ids\n",
    "        self.df_test.set_index('listing_id', inplace=True)\n",
    "\n",
    "        return self.df_cv, self.df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs_sn1 = pickle.load(open('stacker-sn-l1.pkl', 'rb'))\n",
    "\n",
    "dfs_lgbm_v2 = pickle.load(open('modeloutput-klightgbm-clf-r2.pkl', 'rb'))\n",
    "dfs_lgbmr_v2 = pickle.load(open('modeloutput-lightgbm-reg-r2.pkl', 'rb'))\n",
    "\n",
    "dfs_lgbm_v3 = pickle.load(open('modeloutput-klightgbm-clf-r3.pkl', 'rb'))\n",
    "dfs_lgbmr_v3 = pickle.load(open('modeloutput-lightgbm-reg-r3.pkl', 'rb'))\n",
    "\n",
    "dfs_xgbv2 = pickle.load(open('modeloutput-xgb-clf-r2.pkl', 'rb'))\n",
    "dfs_xgbv3 = pickle.load(open('modeloutput-xgb-clf-r3.pkl', 'rb'))\n",
    "dfs_xgbrv2 = pickle.load(open('modeloutput-xgb-reg-r2.pkl', 'rb'))\n",
    "dfs_xgbrv3 = pickle.load(open('modeloutput-xgb-reg-r3.pkl', 'rb'))\n",
    "\n",
    "dfs_rf = pickle.load(open('model-output-rf.pkl', 'rb'))\n",
    "\n",
    "dfs_med = pickle.load(open('model-medium-logdiff.pkl', 'rb'))\n",
    "dfs_med3 = pickle.load(open('model-medium-logdiff-r2.pkl', 'rb'))\n",
    "\n",
    "dfs_nn = pickle.load(open('bag-model-nn-v1.pkl', 'rb'))\n",
    "\n",
    "mset = [dfs_lgbm3, dfs_lgbmr3, dfs_sn1, dfs_rf, dfs_xgbv2, dfs_xgbrv3, dfs_med3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499411831078\n",
      "0.495067023816\n",
      "0.495687755355\n",
      "0.491160383877\n",
      "0.491307543001\n",
      "CV logloss: 0.494527103124\n"
     ]
    }
   ],
   "source": [
    "mset = [dfs_lgbm_v3, dfs_lgbmr_v3, dfs_sn1, dfs_rf, dfs_xgbv3, dfs_xgbrv2, dfs_med3]\n",
    "\n",
    "s = StackerLGBM(mset)\n",
    "df_cv, df_test = s.run(train_ids, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499578324354\n",
      "0.495067394654\n",
      "0.495339775902\n",
      "0.49151814749\n",
      "0.491356163716\n",
      "CV logloss: 0.494572156705\n"
     ]
    }
   ],
   "source": [
    "mset = [dfs_lgbm3, dfs_lgbmr3, dfs_sn1, dfs_rf, dfs_xgbv3, dfs_xgbrv2, dfs_med3]\n",
    "mset = [dfs_lgbmr_v2, dfs_lgbm_v2, dfs_lgbm_v3, dfs_lgbmr_v3, dfs_sn1, dfs_rf, dfs_xgbv3, dfs_xgbrv2, dfs_med]\n",
    "\n",
    "s = StackerLGBM(mset)\n",
    "df_cv, df_test = s.run(train_ids, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09037\tvalid-mlogloss:1.09044\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.667259\tvalid-mlogloss:0.671842\n",
      "[200]\ttrain-mlogloss:0.548597\tvalid-mlogloss:0.556086\n",
      "[300]\ttrain-mlogloss:0.509331\tvalid-mlogloss:0.518978\n",
      "[400]\ttrain-mlogloss:0.49475\tvalid-mlogloss:0.506256\n",
      "[500]\ttrain-mlogloss:0.488376\tvalid-mlogloss:0.501696\n",
      "[600]\ttrain-mlogloss:0.484936\tvalid-mlogloss:0.499931\n",
      "[700]\ttrain-mlogloss:0.482544\tvalid-mlogloss:0.499216\n",
      "[800]\ttrain-mlogloss:0.480674\tvalid-mlogloss:0.498904\n",
      "[900]\ttrain-mlogloss:0.47906\tvalid-mlogloss:0.498848\n",
      "[1000]\ttrain-mlogloss:0.477567\tvalid-mlogloss:0.498777\n",
      "Stopping. Best iteration:\n",
      "[1001]\ttrain-mlogloss:0.477549\tvalid-mlogloss:0.498769\n",
      "\n",
      "0.498769044788\n",
      "[0]\ttrain-mlogloss:1.09038\tvalid-mlogloss:1.0904\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.668235\tvalid-mlogloss:0.670198\n",
      "[200]\ttrain-mlogloss:0.549825\tvalid-mlogloss:0.553129\n",
      "[300]\ttrain-mlogloss:0.510612\tvalid-mlogloss:0.515331\n",
      "[400]\ttrain-mlogloss:0.495925\tvalid-mlogloss:0.502191\n",
      "[500]\ttrain-mlogloss:0.489487\tvalid-mlogloss:0.49729\n",
      "[600]\ttrain-mlogloss:0.486001\tvalid-mlogloss:0.495404\n",
      "[700]\ttrain-mlogloss:0.483499\tvalid-mlogloss:0.494598\n",
      "[800]\ttrain-mlogloss:0.4816\tvalid-mlogloss:0.494299\n",
      "[900]\ttrain-mlogloss:0.479888\tvalid-mlogloss:0.494241\n",
      "Stopping. Best iteration:\n",
      "[867]\ttrain-mlogloss:0.480451\tvalid-mlogloss:0.494211\n",
      "\n",
      "0.494210912498\n",
      "[0]\ttrain-mlogloss:1.09038\tvalid-mlogloss:1.0904\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.667947\tvalid-mlogloss:0.670596\n",
      "[200]\ttrain-mlogloss:0.549475\tvalid-mlogloss:0.553999\n",
      "[300]\ttrain-mlogloss:0.510268\tvalid-mlogloss:0.516339\n",
      "[400]\ttrain-mlogloss:0.495661\tvalid-mlogloss:0.503119\n",
      "[500]\ttrain-mlogloss:0.489261\tvalid-mlogloss:0.497998\n",
      "[600]\ttrain-mlogloss:0.485776\tvalid-mlogloss:0.4958\n",
      "[700]\ttrain-mlogloss:0.48332\tvalid-mlogloss:0.494775\n",
      "[800]\ttrain-mlogloss:0.481366\tvalid-mlogloss:0.494282\n",
      "[900]\ttrain-mlogloss:0.47974\tvalid-mlogloss:0.494022\n",
      "[1000]\ttrain-mlogloss:0.478128\tvalid-mlogloss:0.493941\n",
      "[1100]\ttrain-mlogloss:0.47659\tvalid-mlogloss:0.493915\n",
      "[1200]\ttrain-mlogloss:0.475146\tvalid-mlogloss:0.493862\n",
      "Stopping. Best iteration:\n",
      "[1182]\ttrain-mlogloss:0.475417\tvalid-mlogloss:0.493842\n",
      "\n",
      "0.493841798147\n",
      "[0]\ttrain-mlogloss:1.0904\tvalid-mlogloss:1.09042\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.668419\tvalid-mlogloss:0.669998\n",
      "[200]\ttrain-mlogloss:0.550209\tvalid-mlogloss:0.552417\n",
      "[300]\ttrain-mlogloss:0.511198\tvalid-mlogloss:0.513868\n",
      "[400]\ttrain-mlogloss:0.496709\tvalid-mlogloss:0.499879\n",
      "[500]\ttrain-mlogloss:0.490456\tvalid-mlogloss:0.494365\n",
      "[600]\ttrain-mlogloss:0.487031\tvalid-mlogloss:0.492021\n",
      "[700]\ttrain-mlogloss:0.484682\tvalid-mlogloss:0.490882\n",
      "[800]\ttrain-mlogloss:0.482844\tvalid-mlogloss:0.4904\n",
      "[900]\ttrain-mlogloss:0.481222\tvalid-mlogloss:0.490225\n",
      "[1000]\ttrain-mlogloss:0.479703\tvalid-mlogloss:0.49014\n",
      "[1100]\ttrain-mlogloss:0.478274\tvalid-mlogloss:0.490118\n",
      "Stopping. Best iteration:\n",
      "[1070]\ttrain-mlogloss:0.478702\tvalid-mlogloss:0.490104\n",
      "\n",
      "0.490104121135\n",
      "[0]\ttrain-mlogloss:1.09039\tvalid-mlogloss:1.0904\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.668614\tvalid-mlogloss:0.669112\n",
      "[200]\ttrain-mlogloss:0.55029\tvalid-mlogloss:0.551423\n",
      "[300]\ttrain-mlogloss:0.511276\tvalid-mlogloss:0.513225\n",
      "[400]\ttrain-mlogloss:0.49678\tvalid-mlogloss:0.499617\n",
      "[500]\ttrain-mlogloss:0.490431\tvalid-mlogloss:0.494306\n",
      "[600]\ttrain-mlogloss:0.486922\tvalid-mlogloss:0.492022\n",
      "[700]\ttrain-mlogloss:0.484498\tvalid-mlogloss:0.49093\n",
      "[800]\ttrain-mlogloss:0.482533\tvalid-mlogloss:0.490436\n",
      "[900]\ttrain-mlogloss:0.480841\tvalid-mlogloss:0.490197\n",
      "[1000]\ttrain-mlogloss:0.479222\tvalid-mlogloss:0.490118\n",
      "Stopping. Best iteration:\n",
      "[990]\ttrain-mlogloss:0.479389\tvalid-mlogloss:0.490105\n",
      "\n",
      "0.490105311864\n",
      "CV logloss: 0.493406438343\n"
     ]
    }
   ],
   "source": [
    "s = StackerXGB(mset)\n",
    "df_cv_xgb, df_test_xgb = s.run(train_ids, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09034\tvalid-mlogloss:1.09039\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.667141\tvalid-mlogloss:0.671741\n",
      "[200]\ttrain-mlogloss:0.548445\tvalid-mlogloss:0.556014\n",
      "[300]\ttrain-mlogloss:0.509216\tvalid-mlogloss:0.519055\n",
      "[400]\ttrain-mlogloss:0.494614\tvalid-mlogloss:0.506394\n",
      "[500]\ttrain-mlogloss:0.488227\tvalid-mlogloss:0.501907\n",
      "[600]\ttrain-mlogloss:0.484719\tvalid-mlogloss:0.500111\n",
      "[700]\ttrain-mlogloss:0.482378\tvalid-mlogloss:0.499413\n",
      "[800]\ttrain-mlogloss:0.480513\tvalid-mlogloss:0.499202\n",
      "[900]\ttrain-mlogloss:0.47883\tvalid-mlogloss:0.499092\n",
      "Stopping. Best iteration:\n",
      "[909]\ttrain-mlogloss:0.478687\tvalid-mlogloss:0.499075\n",
      "\n",
      "0.499075419191\n",
      "[0]\ttrain-mlogloss:1.09034\tvalid-mlogloss:1.09035\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.668145\tvalid-mlogloss:0.670296\n",
      "[200]\ttrain-mlogloss:0.549697\tvalid-mlogloss:0.553235\n",
      "[300]\ttrain-mlogloss:0.510534\tvalid-mlogloss:0.515477\n",
      "[400]\ttrain-mlogloss:0.495859\tvalid-mlogloss:0.502325\n",
      "[500]\ttrain-mlogloss:0.489436\tvalid-mlogloss:0.49743\n",
      "[600]\ttrain-mlogloss:0.485891\tvalid-mlogloss:0.495475\n",
      "[700]\ttrain-mlogloss:0.483436\tvalid-mlogloss:0.494767\n",
      "[800]\ttrain-mlogloss:0.481502\tvalid-mlogloss:0.494493\n",
      "[900]\ttrain-mlogloss:0.479793\tvalid-mlogloss:0.494381\n",
      "Stopping. Best iteration:\n",
      "[906]\ttrain-mlogloss:0.479689\tvalid-mlogloss:0.494375\n",
      "\n",
      "0.494375436102\n",
      "[0]\ttrain-mlogloss:1.09033\tvalid-mlogloss:1.09036\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.667801\tvalid-mlogloss:0.670344\n",
      "[200]\ttrain-mlogloss:0.549313\tvalid-mlogloss:0.553658\n",
      "[300]\ttrain-mlogloss:0.510166\tvalid-mlogloss:0.51604\n",
      "[400]\ttrain-mlogloss:0.49557\tvalid-mlogloss:0.502749\n",
      "[500]\ttrain-mlogloss:0.489209\tvalid-mlogloss:0.497621\n",
      "[600]\ttrain-mlogloss:0.485683\tvalid-mlogloss:0.495364\n",
      "[700]\ttrain-mlogloss:0.483278\tvalid-mlogloss:0.494366\n",
      "[800]\ttrain-mlogloss:0.481345\tvalid-mlogloss:0.493814\n",
      "[900]\ttrain-mlogloss:0.479674\tvalid-mlogloss:0.493507\n",
      "[1000]\ttrain-mlogloss:0.478108\tvalid-mlogloss:0.493355\n",
      "[1100]\ttrain-mlogloss:0.476596\tvalid-mlogloss:0.4933\n",
      "[1200]\ttrain-mlogloss:0.475103\tvalid-mlogloss:0.493337\n",
      "Stopping. Best iteration:\n",
      "[1156]\ttrain-mlogloss:0.475776\tvalid-mlogloss:0.493271\n",
      "\n",
      "0.493271210535\n",
      "[0]\ttrain-mlogloss:1.09036\tvalid-mlogloss:1.09039\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.668185\tvalid-mlogloss:0.670168\n",
      "[200]\ttrain-mlogloss:0.549982\tvalid-mlogloss:0.552742\n",
      "[300]\ttrain-mlogloss:0.511043\tvalid-mlogloss:0.5143\n",
      "[400]\ttrain-mlogloss:0.496547\tvalid-mlogloss:0.500347\n",
      "[500]\ttrain-mlogloss:0.490218\tvalid-mlogloss:0.494837\n",
      "[600]\ttrain-mlogloss:0.486758\tvalid-mlogloss:0.492532\n",
      "[700]\ttrain-mlogloss:0.484339\tvalid-mlogloss:0.491468\n",
      "[800]\ttrain-mlogloss:0.482487\tvalid-mlogloss:0.49096\n",
      "[900]\ttrain-mlogloss:0.480788\tvalid-mlogloss:0.490768\n",
      "[1000]\ttrain-mlogloss:0.479238\tvalid-mlogloss:0.490684\n",
      "[1100]\ttrain-mlogloss:0.477761\tvalid-mlogloss:0.49058\n",
      "Stopping. Best iteration:\n",
      "[1123]\ttrain-mlogloss:0.477419\tvalid-mlogloss:0.490548\n",
      "\n",
      "0.490547720044\n",
      "[0]\ttrain-mlogloss:1.09037\tvalid-mlogloss:1.09036\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.668495\tvalid-mlogloss:0.669075\n",
      "[200]\ttrain-mlogloss:0.550153\tvalid-mlogloss:0.551398\n",
      "[300]\ttrain-mlogloss:0.51109\tvalid-mlogloss:0.513201\n",
      "[400]\ttrain-mlogloss:0.496566\tvalid-mlogloss:0.499595\n",
      "[500]\ttrain-mlogloss:0.490205\tvalid-mlogloss:0.494406\n",
      "[600]\ttrain-mlogloss:0.486695\tvalid-mlogloss:0.492126\n",
      "[700]\ttrain-mlogloss:0.484236\tvalid-mlogloss:0.491104\n",
      "[800]\ttrain-mlogloss:0.482305\tvalid-mlogloss:0.49061\n",
      "[900]\ttrain-mlogloss:0.480617\tvalid-mlogloss:0.490458\n",
      "[1000]\ttrain-mlogloss:0.479041\tvalid-mlogloss:0.490421\n",
      "Stopping. Best iteration:\n",
      "[953]\ttrain-mlogloss:0.479784\tvalid-mlogloss:0.490405\n",
      "\n",
      "0.490404864738\n",
      "CV logloss: 0.493535120392\n"
     ]
    }
   ],
   "source": [
    "mset = [dfs_lgbm3, dfs_lgbmr3, dfs_sn1, dfs_rf, dfs_xgbv2, dfs_xgbrv2, dfs_med3]\n",
    "mset = [dfs_lgbmr_v2, dfs_lgbm_v2, dfs_lgbm_v3, dfs_lgbmr_v3, dfs_sn1, dfs_rf, dfs_xgbv3, dfs_xgbrv2, dfs_med]\n",
    "\n",
    "sa = StackerXGB(mset)\n",
    "df_cv_xgba, df_test_xgba = sa.run(train_ids, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low 0.0107192 0.998882\n",
      "medium 0.00080798 0.808739\n",
      "high 0.000283104 0.939767\n"
     ]
    }
   ],
   "source": [
    "for k in df_test_xgb.keys():\n",
    "    print(k, df_test_xgb[k].min(), df_test_xgb[k].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_xgb.to_csv('submission-0424-4a.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare with old submission to make sure this isn't stupid ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_sub = pd.read_csv('../nb/k0423-r2_verywidestack.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_sub.set_index('listing_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9946970694262699"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_xgb.low.corr(old_sub.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98974570691903441"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_xgb.medium.corr(old_sub.medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99106565255506196"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_xgb.high.corr(old_sub.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 410,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
