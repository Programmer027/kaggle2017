{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Talk 3/22/2017\n",
    "\n",
    "This is going to be an overview of some of my Kaggle competitions - in particular the recent Two Sigma Financial Modeling competition.\n",
    "\n",
    "I've been Kaggling since 2015 - I've gotten two #19 finishes (a bit short of a gold medal!) and a few other decent finishes :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Kaggle Competitions\n",
    "\n",
    "There are usually 4-5 competitions running at once - a mix of image-based and statistical competitions.\n",
    "\n",
    "Competitions consist of a training and test set.  In most competitions, the test set is split - during the competition you see only a public subset, with the larger private leaderboard shown after the competition ends.  In some others, a second test set is revealed near the end of the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Two Sigma Financial Modeling\n",
    "\n",
    "This was Kaggle's first code competition, where instead of submitting predictions, Python kernels were uploaded and run on Kaggle's cloud servers.  (Unfortunatly, submissions are now offline and there's no test data yet(?), so there are a few things I'd like to look at but can't right now...)\n",
    "\n",
    "This added a real constraint to all competitiors, since there was a 60 minute runtime limit (using two cores of a ~3ghz Xeon server box)  It was possible to encode relatively small binaries of models (Gilberto Titericz Junior's team did this) but I didn't attempt it myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "- https://www.kaggle.com/c/two-sigma-financial-modeling/data\n",
    "\n",
    "Basically, it was about predicting time series (daily) data on various financial instruments that came and went during the timeframe.  In the middle of the competition, the ID's and timestamps on the server end were changed to prevent having explicit data references in the kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I did\n",
    "\n",
    "The core of my script was initially a blend of a popular kernel using ExtraTreesClassifier, combined with one using XGBoost.  I kept the blend between tree and linear models, since it was helpful with the public leaderboard.\n",
    "\n",
    "The main feature engineering I did involved keeping previous values - which led me to figuring out the technical_20 and 30 connection - even though I didn't explore the +/- relationship, my linear model worked quite well.\n",
    "\n",
    "I tried to be careful to prevent overfitting - picking solutions which were stable on both the second half of the public data, and the public half of the leaderboard data.  Unfortunately, this led to not gaining enough score on the private half, so I went from #12 to #51.  (and my best submissions would have gotten me to about #28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To me, the hardest parts were:\n",
    "\n",
    "- Being blind as to what's in the test set (each time frame - each half of the public data, and the two splits in the private data - were very different).  This not only caused major overfitting issues, but fed into...\n",
    "\n",
    "- Having only two (successful) submissions per day.  There were ways to cheat with intentionally erroring out submissions, but I didn't want to use them.  So it was very easy to have something I thought would be good... and then it wasn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/happycube/two-sigma-financial-modeling/two-sigma-03-shorter-version/run/979213\n",
    "\n",
    "- This is the simplest effective solution I've seen - it created three simple linear models using technical_20 and 30.  It's in the ~130 range on the private Leaderboard (~.02) which is good for a silver medal, and takes only one minute to run on Kaggle's server.  I refactored the code with more effective use of Pandas, bringing it to under 100 lines of code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways/Suggestions:\n",
    "\n",
    "- Always look for a simpler solution!\n",
    "\n",
    "- On a 'blind' competition like this one, don't worry about the public leaderboard _too_ much.\n",
    "    - (and don't get too excited if you wind up in the top 10!)\n",
    "\n",
    "- Don't spend too much time tuning what you already have (unless you're too tired to do something new :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
