{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kaggle Talk 3/22/2017\n",
    "\n",
    "This is going to be an overview of some of my Kaggle competitions - in particular the recent Two Sigma Financial Modeling competition.\n",
    "\n",
    "I've been Kaggling since 2015 - I've gotten two #19 finishes (a bit short of a gold medal!) and a few other decent finishes :)\n",
    "\n",
    "(https://www.kaggle.com/happycube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About Kaggle Competitions\n",
    "\n",
    "There are usually 4-5 competitions running at once - a mix of image-based and statistical competitions.\n",
    "\n",
    "Competitions consist of a training and test set.  In most competitions, the test set is split - during the competition you see only a public subset, with the larger private leaderboard shown after the competition ends.  In some others, a second test set is revealed near the end of the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### About Two Sigma Financial Modeling\n",
    "\n",
    "This was Kaggle's first code competition, where instead of submitting predictions, Python kernels were uploaded and run on Kaggle's cloud servers.  (Unfortunatly, submissions are now offline and there's no test data yet(?), so there are a few things I'd like to look at but can't right now...)\n",
    "\n",
    "This added a real constraint to all competitiors, since there was a 60 minute runtime limit (using two cores of a ~3ghz Xeon server box)  It was possible to encode relatively small binaries of models (Gilberto Titericz Junior's team did this) but I didn't attempt it myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data overview\n",
    "\n",
    "- https://www.kaggle.com/c/two-sigma-financial-modeling/data\n",
    "\n",
    "> This dataset contains anonymized features pertaining to a time-varying\n",
    "> value for a financial instrument. Each instrument has an id. Time is\n",
    "> represented by the 'timestamp' feature and the variable to predict is\n",
    "> 'y'. No further information will be provided on the meaning of the\n",
    "> features, the transformations that were applied to them, the\n",
    "> timescale, or the type of instruments that are included in the data.\n",
    "\n",
    "Basically, it was about predicting time series (daily) data on various financial instruments that came and went during the timeframe.  In the middle of the competition, the ID's and timestamps on the server end were changed to prevent having explicit data references in the kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric\n",
    "\n",
    "https://www.kaggle.com/c/two-sigma-financial-modeling#evaluation - the R metric was used (in a sense, ironic since R kernels were not supported by the code competition ;) \n",
    "\n",
    "The only view into the test set was the R score of the previous day.  However, this was flawed because it shifts considerably depending on the average y score of that day, while for the whole result the average of *all* days were used.  We will see the stark difference when evaluating the performance of models later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The main feature(s)\n",
    "\n",
    "- While no direct information was revealed, it didn't take long to learn that technical_20 and 30 were the two most influential.  For a while, it was believed that t30 broke down on the training set, so 20 was the most trusted.\n",
    "\n",
    "\n",
    "- I stumbled upon the truth when I did a reverse time travel experiment, fitting to the previous day's y value (pandas group function is very good for doing this) - I was able to work out a linear model using 20, 30 and their previous values.  I was able to get a .8 R score between them and the previous day's y, which provided a major source of data.\n",
    "\n",
    "\n",
    "- cc05 revealed what their actual meanings were (I just enjoyed the results of the linear model and didn't look too closely ;) ) - https://www.kaggle.com/chenjx1005/two-sigma-financial-modeling/physical-meanings-of-technical-20-30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What I did\n",
    "\n",
    "- https://github.com/happycube/kaggle2017/blob/master/twosigma/sub-227738.ipynb\n",
    "\n",
    "The core of my script was initially a blend of a popular kernel using ExtraTreesClassifier, combined with one using XGBoost, along with Pandas manipulation to keep previous values to provide more features.\n",
    "\n",
    "I tried to be careful to prevent overfitting - picking solutions which were stable on both the second half of the public data, and the public half of the leaderboard data.  Unfortunately, this led to not gaining enough score on the private half, so I went from #12 to #51.  (and my best submissions would have gotten me to about #28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The hardest parts\n",
    "\n",
    "- Being blind as to what's in the test set (each time frame - each half of the public data, and the two splits in the private data - were very different).  This not only caused major overfitting issues, but fed into...\n",
    "\n",
    "- Having only two (successful) submissions per day.  There were ways to cheat with intentionally erroring out submissions, but I didn't want to use them.  So it was very easy to have something I thought would be good... and then it wasn't.\n",
    "\n",
    "And my biggest mistake was assuming the private LB would act like an extrapolation of the public LB.  In the end I gained some score, but not nearly as much as the top performers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other solutions:\n",
    "\n",
    "- https://www.kaggle.com/phegde/two-sigma-financial-modeling/team-pradeep-arthur-private-lb-10-solution/code\n",
    "    - #10 private LB:  This evolved a popular (if complex) starter script into a diverse ensemble.\n",
    "    \n",
    "    \n",
    "- https://www.kaggle.com/tks0123456789/two-sigma-financial-modeling/xgb-500-600-001\n",
    "    - #16 private LB - and #525 public!  This kernel was both simple and daring - it uses a mixture of eight XGBoost models with *no* feature engineering.\n",
    "\n",
    "\n",
    "- https://www.kaggle.com/happycube/two-sigma-financial-modeling/two-sigma-03-shorter-version/run/979213\n",
    "    - This is the simplest effective solution I've seen - it created three simple linear models using technical_20 and 30.  It's in the ~130 range on the private Leaderboard (~.02) which is good for a silver medal, and takes only one minute to run on Kaggle's server.  I refactored the code with more effective use of Pandas, bringing it to under 100 lines of code.\n",
    "\n",
    "paulperry did more research: https://github.com/paulperry/kaggle/blob/master/two-sigma-financial-modeling/models/other_models.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways/Suggestions:\n",
    "\n",
    "- Always look for a simpler solution!\n",
    "\n",
    "- On a 'blind' competition like this one, don't worry about the public leaderboard _too_ much.\n",
    "    - (and don't get too excited if you wind up in the top 10!)\n",
    "\n",
    "- Don't spend too much time tuning what you already have (unless you're too tired to do something new :) )\n",
    "\n",
    "- (And write up notes right *after* the end of the competition, even if you're tired.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Oussama Errabia added:\n",
    "\n",
    "- When ensembling, make sure the various members of the ensemble are different, to take advantage of different parts of the data.\n",
    "\n",
    "- Sometimes, it's not all about generalization.  (I can speak to that!)\n",
    "\n",
    "- Trust your CV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
